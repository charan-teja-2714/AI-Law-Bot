# üèóÔ∏è MediSense System Architecture & Concepts

## üìã Table of Contents
1. [System Overview](#system-overview)
2. [Architecture Layers](#architecture-layers)
3. [Core Concepts Explained](#core-concepts-explained)
4. [Data Flow](#data-flow)
5. [Technology Stack](#technology-stack)
6. [Implemented Features](#implemented-features)
7. [Not Implemented](#not-implemented)
8. [Key Design Decisions](#key-design-decisions)

---

## üéØ System Overview

**MediSense** is a medical report assistant that uses RAG (Retrieval-Augmented Generation) to help patients understand their medical reports in simple language.

### What It Does
- ‚úÖ Uploads PDF medical reports
- ‚úÖ Extracts text (including OCR for scanned PDFs)
- ‚úÖ Answers questions about the report
- ‚úÖ Generates questions to ask your doctor
- ‚úÖ Provides patient-friendly or doctor-level explanations

### What It Does NOT Do
- ‚ùå Does NOT diagnose diseases
- ‚ùå Does NOT prescribe treatments
- ‚ùå Does NOT replace medical professionals
- ‚ùå Does NOT store patient data permanently

---

## üèõÔ∏è Architecture Layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRESENTATION LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ              React Frontend (Port 3000)             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ User Interface (ChatGPT-style)                   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ File Upload                                      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Chat Display                                     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Theme Toggle (Light/Dark)                        ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì HTTP REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     APPLICATION LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ           FastAPI Backend (Port 8000)               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ API Endpoints                                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Request Validation                               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Session Management                               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Business Logic Orchestration                     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SERVICE LAYER                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ              Core AI Services                       ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ PDF Processing (PyPDF2 + Tesseract)             ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Text Chunking (Medical-aware)                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Vector Embeddings (sentence-transformers)        ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Retrieval (Hybrid: Dense + BM25)                 ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Generation (Groq LLM)                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Question Generation                              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       DATA LAYER                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   SQLite     ‚îÇ  ‚îÇ   Pinecone   ‚îÇ  ‚îÇ   Groq LLM   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ (Chat Hist.) ‚îÇ  ‚îÇ  (Vectors)   ‚îÇ  ‚îÇ (Generation) ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß† Core Concepts Explained

### 1. RAG (Retrieval-Augmented Generation)

**What is it?**
RAG combines information retrieval with text generation to provide accurate, context-aware answers.

**How it works in MediSense:**
```
User Question: "What is my hemoglobin level?"
        ‚Üì
Step 1: RETRIEVE relevant chunks from PDF
        ‚Üí "Hemoglobin: 13.5 g/dL (Normal: 12-16)"
        ‚Üì
Step 2: AUGMENT the question with retrieved context
        ‚Üí Question + Retrieved Text
        ‚Üì
Step 3: GENERATE answer using LLM
        ‚Üí "Your hemoglobin level is 13.5 g/dL, which is 
           within the normal range of 12-16 g/dL."
```

**Why RAG?**
- ‚úÖ Accurate: Uses actual report data
- ‚úÖ Contextual: Understands the specific report
- ‚úÖ Safe: Doesn't hallucinate information
- ‚úÖ Explainable: Can trace back to source

---

### 2. Vector Embeddings

**What are they?**
Mathematical representations of text that capture semantic meaning.

**Example:**
```
Text: "Hemoglobin level is low"
Embedding: [0.23, -0.45, 0.67, ..., 0.12]  (768 numbers)

Text: "Hb count is decreased"
Embedding: [0.21, -0.43, 0.65, ..., 0.14]  (similar numbers!)
```

**Why use them?**
- Finds similar meanings, not just exact words
- "low hemoglobin" matches "decreased Hb"
- Enables semantic search

**In MediSense:**
- Model: `sentence-transformers/all-mpnet-base-v2`
- Dimension: 768
- Stored in: Pinecone vector database

---

### 3. Hybrid Retrieval (Dense + BM25)

**Two search methods combined:**

**Dense Search (Vector-based):**
```
Query: "blood sugar levels"
Finds: "glucose concentration" (semantic match)
```

**BM25 (Keyword-based):**
```
Query: "HbA1c"
Finds: exact term "HbA1c" in report
```

**Why both?**
- Dense: Understands meaning
- BM25: Finds exact medical terms
- Combined: Best of both worlds

---

### 4. Medical-Aware Chunking

**What is chunking?**
Breaking long documents into smaller pieces for processing.

**Why medical-aware?**
```
‚ùå Bad Chunking:
Chunk 1: "Hemoglobin: 13.5"
Chunk 2: "g/dL (Normal: 12-16)"
‚Üí Context is split!

‚úÖ Medical-Aware Chunking:
Chunk 1: "Hemoglobin: 13.5 g/dL (Normal: 12-16)"
‚Üí Complete information preserved!
```

**How it works:**
- Detects lab result patterns: `Name: Value Unit`
- Keeps related lines together
- Chunk size: 800 characters
- Overlap: 150 characters (for context)

---

### 5. Query Rewriting

**What is it?**
Improving user questions before searching.

**Example:**
```
User: "What's my sugar?"
        ‚Üì Rewrite
Better: "What is the glucose level in this medical report?"
        ‚Üì Search
Finds: "Glucose: 95 mg/dL"
```

**Why?**
- More specific queries = better retrieval
- Adds medical context
- Improves search accuracy

---

### 6. Re-ranking

**What is it?**
Sorting search results by relevance.

**Process:**
```
Step 1: Retrieve 12 chunks from Pinecone
Step 2: Calculate similarity scores
Step 3: Sort by score
Step 4: Return top 5 most relevant
```

**Why?**
- Ensures best context for LLM
- Reduces noise
- Improves answer quality

---

### 7. Safety-First Prompting

**Core Principle:**
The system NEVER diagnoses or prescribes.

**Prompt Structure:**
```
System Instructions:
- Do NOT diagnose diseases
- Do NOT suggest treatments
- Do NOT predict outcomes
- Explain what the report shows
- Use simple language (patient mode)
- Be reassuring but factual
```

**Example:**
```
‚ùå Bad: "You have diabetes. Take metformin."
‚úÖ Good: "Your glucose level is 180 mg/dL, which is above 
         the normal range. Please discuss this with your 
         doctor for proper evaluation."
```

---

## üîÑ Data Flow

### PDF Upload Flow
```
1. User uploads PDF
   ‚Üì
2. Frontend ‚Üí POST /api/upload-pdf
   ‚Üì
3. Backend receives file
   ‚Üì
4. PDF Processing:
   a. Extract text (PyPDF2)
   b. If scanned ‚Üí OCR (Tesseract)
   c. Medical-aware chunking
   ‚Üì
5. Generate embeddings (sentence-transformers)
   ‚Üì
6. Store in Pinecone:
   - Dense vectors (768-dim)
   - BM25 sparse vectors
   - Metadata (source, page, text)
   ‚Üì
7. Return success to frontend
   ‚Üì
8. Display "‚úì filename.pdf" in header
```

### Chat Flow
```
1. User types question
   ‚Üì
2. Frontend ‚Üí POST /api/chat
   ‚Üì
3. Backend:
   a. Rewrite query (LLM)
   b. Retrieve chunks (Pinecone + BM25)
   c. Re-rank by relevance
   d. Generate answer (Groq LLM)
   ‚Üì
4. Save to SQLite:
   - User message
   - AI response
   - Timestamp
   ‚Üì
5. Return response to frontend
   ‚Üì
6. Display in chat UI
```

### Question Generation Flow
```
1. User clicks "Generate Questions"
   ‚Üì
2. Frontend ‚Üí POST /api/generate-questions
   ‚Üì
3. Backend:
   a. Query Pinecone for key findings
   b. Extract context
   c. Generate questions (LLM)
      - ONLY questions
      - NO answers
      - NO diagnosis
   ‚Üì
4. Return questions
   ‚Üì
5. Display in modal dialog
```

---

## üõ†Ô∏è Technology Stack

### Frontend (React)
```
React 18.2.0          ‚Üí UI framework
JavaScript ES6+       ‚Üí Language
CSS3                  ‚Üí Styling
Fetch API             ‚Üí HTTP client
```

**Why React?**
- Modern, component-based
- Fast, responsive UI
- Large ecosystem
- Easy to maintain

---

### Backend (FastAPI)
```
FastAPI               ‚Üí Web framework
Uvicorn               ‚Üí ASGI server
Pydantic              ‚Üí Data validation
Python 3.8+           ‚Üí Language
```

**Why FastAPI?**
- Fast (async support)
- Auto-generated API docs
- Type safety
- Easy to test

---

### AI/ML Stack
```
LangChain             ‚Üí LLM orchestration
Groq (llama-3.3-70b)  ‚Üí Text generation
sentence-transformers ‚Üí Embeddings
Pinecone              ‚Üí Vector database
BM25                  ‚Üí Keyword search
PyPDF2                ‚Üí PDF parsing
Tesseract             ‚Üí OCR
```

**Why these choices?**

**Groq:**
- Fast inference
- High-quality responses
- Cost-effective

**Pinecone:**
- Managed vector DB
- Fast similarity search
- Scalable

**sentence-transformers:**
- Pre-trained models
- Good for medical text
- 768-dim embeddings

---

### Database
```
SQLite                ‚Üí Chat history
```

**Why SQLite?**
- Lightweight
- No setup required
- File-based
- Easy to migrate to PostgreSQL later

**What's stored:**
- Session ID
- User messages
- AI responses
- Timestamps
- User role (patient/doctor)

**What's NOT stored:**
- PDFs (too large)
- Embeddings (in Pinecone)
- Personal health data

---

## ‚úÖ Implemented Features

### 1. PDF Processing
- ‚úÖ Text extraction (PyPDF2)
- ‚úÖ OCR for scanned PDFs (Tesseract)
- ‚úÖ Medical-aware chunking
- ‚úÖ Metadata preservation (page numbers)

### 2. Vector Search
- ‚úÖ Dense embeddings (sentence-transformers)
- ‚úÖ Sparse BM25 encoding
- ‚úÖ Hybrid retrieval
- ‚úÖ Re-ranking by relevance

### 3. Chat Interface
- ‚úÖ ChatGPT-style UI
- ‚úÖ Real-time responses
- ‚úÖ Message history
- ‚úÖ Loading indicators
- ‚úÖ Error handling

### 4. Role-Based Explanations
- ‚úÖ Patient mode (simple language)
- ‚úÖ Doctor mode (technical terms)
- ‚úÖ Dynamic prompt adjustment

### 5. Question Generation
- ‚úÖ Context-aware questions
- ‚úÖ Safety-first (no diagnosis)
- ‚úÖ Modal dialog display
- ‚úÖ Formatted bullet points

### 6. UI Features
- ‚úÖ Sidebar with chat history
- ‚úÖ File upload in input bar
- ‚úÖ Light/Dark theme toggle
- ‚úÖ Mobile responsive
- ‚úÖ PDF upload indicator
- ‚úÖ Smooth animations

### 7. Session Management
- ‚úÖ Unique session IDs
- ‚úÖ In-memory state
- ‚úÖ SQLite persistence
- ‚úÖ Chat history storage

---

## ‚ùå Not Implemented

### Features NOT in Current Version
- ‚ùå User authentication
- ‚ùå Multi-user support
- ‚ùå Image upload processing
- ‚ùå Voice input/output
- ‚ùå Report comparison
- ‚ùå Trend analysis over time
- ‚ùå Email notifications
- ‚ùå PDF export of chat
- ‚ùå Multi-language support
- ‚ùå Mobile app

### Why Not?
- **Scope:** MVP focused on core RAG functionality
- **Complexity:** Would require significant additional work
- **Not Requested:** User didn't ask for these features

---

## üéØ Key Design Decisions

### 1. Why React + FastAPI (not Streamlit)?

**Streamlit (Old):**
- ‚úÖ Fast prototyping
- ‚ùå Limited UI customization
- ‚ùå Page reloads on interaction
- ‚ùå No API for other clients

**React + FastAPI (New):**
- ‚úÖ Modern, responsive UI
- ‚úÖ RESTful API
- ‚úÖ Scalable architecture
- ‚úÖ Can serve mobile apps later

---

### 2. Why In-Memory Sessions (not Redis)?

**Current:**
```python
self.sessions = {}  # session_id -> {index, pdf_name}
```

**Why?**
- Simple for MVP
- Fast access
- No external dependencies

**Trade-off:**
- Lost on server restart
- Not suitable for multiple instances

**Future:**
- Move to Redis for production
- Enable horizontal scaling

---

### 3. Why SQLite (not PostgreSQL)?

**SQLite:**
- ‚úÖ Zero setup
- ‚úÖ File-based
- ‚úÖ Perfect for chat history
- ‚úÖ Easy to migrate later

**When to upgrade:**
- Multiple concurrent users
- Need for complex queries
- Production deployment

---

### 4. Why Pinecone (not local vector DB)?

**Pinecone:**
- ‚úÖ Managed service
- ‚úÖ Fast similarity search
- ‚úÖ Scalable
- ‚úÖ No infrastructure management

**Alternatives:**
- Chroma (local, open-source)
- Weaviate (self-hosted)
- FAISS (library, not DB)

**Trade-off:**
- Requires API key
- Costs money at scale
- External dependency

---

### 5. Why Groq (not OpenAI)?

**Groq:**
- ‚úÖ Fast inference
- ‚úÖ Good quality (llama-3.3-70b)
- ‚úÖ Cost-effective

**OpenAI:**
- ‚úÖ Higher quality (GPT-4)
- ‚ùå More expensive
- ‚ùå Slower

**Decision:**
- Groq sufficient for medical explanations
- Can switch to OpenAI if needed

---

## üîê Security & Privacy

### What's Secure
- ‚úÖ API keys in `.env` (not in code)
- ‚úÖ No permanent PDF storage
- ‚úÖ Session-based isolation
- ‚úÖ No personal data in database

### What's NOT Secure (MVP)
- ‚ùå No user authentication
- ‚ùå No encryption at rest
- ‚ùå No HTTPS (local only)
- ‚ùå No rate limiting

### For Production
- Add authentication (JWT)
- Enable HTTPS
- Encrypt sensitive data
- Add rate limiting
- Implement audit logs

---

## üìä Performance Considerations

### Current Performance
- PDF upload: ~5-10 seconds
- Chat response: ~2-3 seconds
- Question generation: ~3-5 seconds

### Bottlenecks
1. **PDF Processing:** OCR is slow
2. **Embedding Generation:** CPU-intensive
3. **LLM Inference:** Network latency

### Optimizations
- ‚úÖ Async processing (FastAPI)
- ‚úÖ Caching embeddings (Pinecone)
- ‚úÖ Re-ranking (reduces LLM context)

### Future Optimizations
- Batch processing
- GPU acceleration
- CDN for frontend
- Caching layer (Redis)

---

## üß™ Testing Strategy

### What Should Be Tested
1. **PDF Processing:**
   - Text extraction
   - OCR fallback
   - Chunking logic

2. **Retrieval:**
   - Vector search accuracy
   - BM25 keyword matching
   - Re-ranking quality

3. **Generation:**
   - Answer relevance
   - Safety (no diagnosis)
   - Language simplicity

4. **API:**
   - Endpoint responses
   - Error handling
   - Session management

### Not Tested (MVP)
- Unit tests
- Integration tests
- Load testing
- Security testing

---

## üöÄ Deployment Considerations

### Current (Local Development)
```
Frontend: localhost:3000
Backend:  localhost:8000
Database: SQLite file
```

### Production Deployment

**Frontend:**
- Build: `npm run build`
- Deploy to: Vercel, Netlify, or CDN
- Environment: Production API URL

**Backend:**
- Containerize: Docker
- Deploy to: AWS, GCP, or Azure
- Use: Gunicorn + Uvicorn workers
- Scale: Multiple instances + Load balancer

**Database:**
- Upgrade: SQLite ‚Üí PostgreSQL
- Add: Redis for sessions
- Backup: Regular snapshots

---

## üìà Scalability Path

### Current (Single User)
```
1 Frontend instance
1 Backend instance
1 SQLite file
```

### Future (Multi-User)
```
Frontend: CDN (global)
Backend:  Multiple instances + Load balancer
Cache:    Redis (sessions)
Database: PostgreSQL (chat history)
Queue:    Celery (async PDF processing)
```

---

## üéì Learning Resources

### RAG Concepts
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)

### Vector Embeddings
- [sentence-transformers Documentation](https://www.sbert.net/)
- [Understanding Embeddings](https://platform.openai.com/docs/guides/embeddings)

### FastAPI
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)

### React
- [React Documentation](https://react.dev/)
- [React Hooks Guide](https://react.dev/reference/react)

---

## ü§ù Contributing Guidelines

### Before Adding Features
1. Discuss with team
2. Check if it aligns with project goals
3. Consider security implications
4. Plan for testing

### Code Standards
- Python: PEP 8
- JavaScript: ESLint
- Comments: Explain WHY, not WHAT
- Functions: Single responsibility

---

## üìû Support & Debugging

### Common Issues

**1. "No module named 'fastapi'"**
```bash
cd backend
pip install -r requirements.txt
```

**2. "Cannot connect to backend"**
- Check backend is running
- Verify port 8000 is free
- Check CORS settings

**3. "PDF upload fails"**
- Check Pinecone API key
- Verify PDF is valid
- Check backend logs

**4. "Questions not generating"**
- Ensure PDF is uploaded
- Check Groq API key
- Verify session exists

---

## üéØ Summary

**MediSense** is a RAG-based medical report assistant that:
- Uses modern React + FastAPI architecture
- Implements hybrid retrieval (dense + sparse)
- Provides safety-first, patient-friendly explanations
- Does NOT diagnose or prescribe
- Designed for MVP, scalable for production

**Key Strengths:**
- Accurate (uses actual report data)
- Safe (no diagnosis/treatment)
- User-friendly (ChatGPT-style UI)
- Extensible (clean architecture)

**Limitations:**
- Single-user (for now)
- No authentication
- Local deployment only
- MVP feature set

**Next Steps:**
- Add authentication
- Deploy to cloud
- Add more features (if requested)
- Improve testing coverage
